---
title: "Assessing prediction uncertainty with the bootstrap"
author: "Alex Hayes"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assessing prediction uncertainty with the bootstrap}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE
)
```

This vignette walks demonstrates how to estimate uncertainty in predictions via the bootstrap. We use [`rsample`](https://tidymodels.github.io/rsample/) throughout. First we show how to calculate confidence intervals, and then how to calculate predictive intervals. If you are unfamiliar with the difference between these two, we highly recommend that you read `vignette("intervals", package = "safepredict")`.

Throughout this vignette we use the nonparametric bootstrap, which is more robust than the parametric bootstrap and does not require us to assumption that we have a correctly specified model. The parametric bootstrap will give you tighter intervals than the procedures we outline here, but we recommend against this unless you are  *very, very* certain that you have correctly specified your model.

This vignette assumes you are interested in a continuous outcome.

## Bootstrapped confidence intervals

Let $X$ be the original data (containing both predictors and outcome).

1. Sample the rows of $X$ with replacement $1, ..., B$ times to create bootstrapped data sets $X_1^*, ..., X_B^*$.
2. Fit your model of choice on each bootstrapped data set and obtain fits $\hat f_1, ..., \hat f_B$.
3. Predict the mean at $X$ with each $\hat f_i$ to get samples from the sampling distribution of $f(X)$.
4. Look at the appropriate quantiles of $f(X)$. You're done!

Let's work through an example, using `glmnet` for a binary classification problem. Our goal will be to predict `Attrition` based on 30 predictors variables.

```{r}
library(dplyr)
library(rsample)

set.seed(27)

attrition <- attrition %>% 
  sample_n(500)

glimpse(attrition)
```

Since we're using `glmnet`, we have to start with a bunch of preprocessing. The `recipes` package makes this sane.

```{r}
library(recipes)

rec <- recipe(Attrition ~ ., data = attrition) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>% 
  step_dummy(all_nominal(), -Attrition) %>% 
  prep()

rec

x <- juice(rec, all_predictors(), composition = "matrix")
y <- juice(rec, all_outcomes())$Attrition
```

We can now fit an L1 penalized logistic regression model, and use `safe_predict()` to calculate residuals.

```{r}
library(glmnet)
library(safepredict)

fit <- cv.glmnet(x, y, family = "binomial")
pred <- safe_predict(fit, x, type = "prob")
```

Let's take a quick look to sanity check our work. We'll plot the estimated probability of attrition versus scaled distanced from home.

```{r}
library(ggplot2)

x %>% 
  as_tibble() %>% 
  bind_cols(pred) %>% 
  ggplot(aes(DistanceFromHome, .pred_Yes)) +
  geom_jitter(alpha = 0.5) +
  theme_bw()
```

This passes the sanity check, we proceed to the bootstrapping.

```{r}
library(purrr)
library(tidyr)

boots <- bootstraps(attrition, times = 10, strata = "Attrition")

fits <- boots %>% 
  mutate(
    prepped = map(splits, prepper, rec),
    x_train = map(prepped, ~juice(.x, all_predictors(), composition = "matrix")),
    y_train = map(prepped, ~juice(.x, all_outcomes())$Attrition),
    model = map2(x_train, y_train, cv.glmnet, family = "binomial")
  )
```

Next we get predictions for each bootstrapped fit

```{r}
boot_preds <- fits %>% 
  mutate(
    preds = map(model, safe_predict, x, type = "prob"),
    preds = map(preds, add_id_column)
  ) %>% 
  unnest(preds, .id = "model")
```

and all that remains to calculate a 90 percent confidence interval is to look at the quantiles of the bootstrapped fits:

```{r}
pred_ci <- boot_preds %>% 
  group_by(.id) %>% 
  summarize(
    .pred_Yes_lower = quantile(.pred_Yes, 0.05),
    .pred_Yes_upper = quantile(.pred_Yes, 0.95)
  ) %>% 
  bind_cols(pred) %>% 
  select(-.pred_No)

pred_ci
```

